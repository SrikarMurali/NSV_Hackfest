{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenalo37/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['hi', 'bye', ['abusive', 'control'], 'boy', 'anxiety', 'assault', ['rape', 'them']]\n",
    "w2v = np.zeros([len(word_list), 6, 300])\n",
    "\n",
    "c1 = 0\n",
    "for word in word_list:\n",
    "    x = np.zeros([6, 300])\n",
    "    if type(word) == list:\n",
    "        c2 = 0\n",
    "        for w in word:\n",
    "            x[c2] = model[w]\n",
    "            c2 += 1\n",
    "    elif type(word) == str:\n",
    "        x[0] = model[word]\n",
    "    w2v[c1] = x\n",
    "    c1 += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.16503906  0.20410156  0.13085938 ... -0.39257812 -0.19238281\n",
      "    0.05786133]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[-0.03027344  0.06542969  0.09228516 ... -0.56640625 -0.0062561\n",
      "    0.15136719]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[-0.38476562 -0.04956055 -0.03125    ... -0.00086212 -0.12890625\n",
      "    0.16894531]\n",
      "  [ 0.14941406  0.06738281 -0.09277344 ...  0.01977539 -0.171875\n",
      "   -0.11279297]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.25        0.09863281 -0.2265625  ...  0.22363281  0.16796875\n",
      "   -0.02893066]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.21777344  0.06176758  0.14355469 ... -0.13574219  0.02099609\n",
      "    0.10644531]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.02990723  0.21386719  0.30273438 ... -0.07177734  0.03112793\n",
      "    0.11230469]\n",
      "  [ 0.03491211  0.08496094  0.0625     ... -0.01904297 -0.00891113\n",
      "    0.02209473]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros([6, 300])\n",
    "x[2:x.shape[0], :].shape\n",
    "type(word_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "bye\n",
      "abusive\n",
      "control\n",
      "boy\n",
      "anxiety\n",
      "assault\n",
      "rape\n",
      "them\n"
     ]
    }
   ],
   "source": [
    "for word in word_list:\n",
    "    x = np.zeros([6, 300])\n",
    "    if type(word) == list:\n",
    "        for w in word:\n",
    "            print(w)\n",
    "    elif type(word) == str:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare data for training\n",
    "\n",
    "# read semi-clean data\n",
    "import pandas as pd\n",
    "doc_complete = pd.read_csv('/Users/kenalo37/hackfest/behavior.csv')\n",
    "doc_complete = list(doc_complete.iloc[:,1])\n",
    "\n",
    "doc = [d.split(' ') for d in doc_complete]\n",
    "for i in range(len(doc)):\n",
    "    temp = [x for x in doc[i] if x!= '' and x.isalpha()]\n",
    "    doc[i] = temp\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    \"\"\"lemmatize and clean doc\"\"\"\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "lemmatized_list = []\n",
    "for d in doc:\n",
    "    assert type(d) == list\n",
    "    if type(d) == list:\n",
    "        temp = [clean(x) for x in d]\n",
    "        lemmatized_list.append(temp)\n",
    "\n",
    "for i in range(len(lemmatized_list)):\n",
    "    temp = [x for x in lemmatized_list[i] if x!= '' and x.isalpha()]\n",
    "    lemmatized_list[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['chinese', 'fire', 'drill'],\n",
       " ['abusive', 'cycle'],\n",
       " ['action', 'assembly', 'theory'],\n",
       " ['active'],\n",
       " ['adoption'],\n",
       " ['age'],\n",
       " ['aggresion', 'towards', 'object'],\n",
       " ['aggressive'],\n",
       " ['alcohol', 'use'],\n",
       " ['alienation'],\n",
       " ['alloplastic', 'adaptation'],\n",
       " ['always', 'never', 'statement'],\n",
       " ['ambitious'],\n",
       " ['anger'],\n",
       " ['animal', 'abuse'],\n",
       " ['animal', 'spirit'],\n",
       " ['argumentative'],\n",
       " ['asccismus'],\n",
       " ['assault', 'gun'],\n",
       " ['assault', 'knife'],\n",
       " ['assertive'],\n",
       " ['attention', 'deficit', 'hyperactivity', 'disorder'],\n",
       " ['attention', 'seeking'],\n",
       " ['attitude', 'change'],\n",
       " ['autoplastic', 'defense', 'mechanism'],\n",
       " ['avoidance'],\n",
       " ['avoidance', 'coping'],\n",
       " ['bad', 'coping'],\n",
       " ['bad', 'habit'],\n",
       " ['baiting'],\n",
       " ['beat'],\n",
       " ['beat'],\n",
       " ['behavior'],\n",
       " ['behavior', 'change'],\n",
       " ['behavioral', 'confirmation'],\n",
       " ['behavioral', 'contagion'],\n",
       " ['behavioral', 'modernity'],\n",
       " ['behavioral', 'urbanism'],\n",
       " ['belittle', 'partner'],\n",
       " ['belittling', 'condescending', 'patronizing'],\n",
       " ['bend', 'finger'],\n",
       " ['binge', 'drinking'],\n",
       " ['bite'],\n",
       " ['blaming'],\n",
       " ['bomb', 'making'],\n",
       " ['boreout'],\n",
       " ['bossy'],\n",
       " ['break', 'girlfriend'],\n",
       " ['breastfeeding'],\n",
       " ['brodie', 'law'],\n",
       " ['bullied'],\n",
       " ['bullying'],\n",
       " ['burn'],\n",
       " ['calculus', 'concept'],\n",
       " ['careless'],\n",
       " ['catastrophizing'],\n",
       " ['cautious'],\n",
       " ['chaos', 'manufacture'],\n",
       " ['cheating'],\n",
       " ['checking', 'phone', 'without', 'permission'],\n",
       " ['choke'],\n",
       " ['chronic', 'broken', 'promise'],\n",
       " ['cigarette', 'use'],\n",
       " ['circular', 'conversation'],\n",
       " ['civil', 'inattention'],\n",
       " ['close', 'minded'],\n",
       " ['clothing', 'change'],\n",
       " ['coerced', 'sex'],\n",
       " ['cold', 'foot'],\n",
       " ['communication', 'theory'],\n",
       " ['conceited'],\n",
       " ['conduct', 'disorder'],\n",
       " ['confirmation', 'bias'],\n",
       " ['conscientious'],\n",
       " ['consistently', 'argue', 'parent'],\n",
       " ['control', 'partner'],\n",
       " ['control', 'syndrome'],\n",
       " ['controlling'],\n",
       " ['coping'],\n",
       " ['creative'],\n",
       " ['cruelty', 'animal'],\n",
       " ['cultural', 'divide'],\n",
       " ['cultural', 'mediation'],\n",
       " ['cultural', 'universal', 'also', 'human', 'universal'],\n",
       " ['curious'],\n",
       " ['cursory'],\n",
       " ['cutting', 'line'],\n",
       " ['death', 'obsessed', 'music'],\n",
       " ['deceitful'],\n",
       " ['defiant'],\n",
       " ['delusion'],\n",
       " ['demand', 'sex'],\n",
       " ['denial'],\n",
       " ['dependency'],\n",
       " ['depression'],\n",
       " ['dissociation'],\n",
       " ['docile'],\n",
       " ['listen', 'adult'],\n",
       " ['domestic', 'abuse'],\n",
       " ['domestic', 'theft'],\n",
       " ['domestic', 'violence'],\n",
       " ['domineering'],\n",
       " ['drunk'],\n",
       " ['dump', 'car'],\n",
       " ['early', 'childhood', 'abuse'],\n",
       " ['easily', 'frustrated'],\n",
       " ['emotional', 'abuse'],\n",
       " ['emotional', 'blackmail'],\n",
       " ['emotional', 'outburst'],\n",
       " ['engulfment'],\n",
       " ['escape', 'fantasy'],\n",
       " ['excess', 'fear'],\n",
       " ['excessive', 'jealousy'],\n",
       " ['excessive', 'worry', 'weight', 'gain'],\n",
       " ['explosive', 'anger'],\n",
       " ['explosive', 'temper'],\n",
       " ['express', 'harming', 'medium'],\n",
       " ['expulsion'],\n",
       " ['extreme', 'insecurity'],\n",
       " ['extreme', 'jealousy'],\n",
       " ['extremist', 'view'],\n",
       " ['extrovert'],\n",
       " ['extroverted'],\n",
       " ['failing', 'school'],\n",
       " ['false', 'accusation'],\n",
       " ['family', 'conflict'],\n",
       " ['fast', 'paced', 'high', 'energy'],\n",
       " ['favoritism', 'scapegoating'],\n",
       " ['fear', 'abandonment'],\n",
       " ['feeling', 'emptiness'],\n",
       " ['feel', 'inferior'],\n",
       " ['friend'],\n",
       " ['force', 'intercourse'],\n",
       " ['force', 'kiss'],\n",
       " ['force', 'penetration'],\n",
       " ['force', 'sex'],\n",
       " ['force', 'touch'],\n",
       " ['frequent', 'pornography', 'usage'],\n",
       " ['friend', 'aggresive', 'child'],\n",
       " ['frivolous', 'litigation'],\n",
       " ['gang'],\n",
       " ['gaslighting'],\n",
       " ['grab', 'clothing'],\n",
       " ['grab', 'face'],\n",
       " ['grooming'],\n",
       " ['gun', 'owner', 'family'],\n",
       " ['hair', 'pull'],\n",
       " ['harassment'],\n",
       " ['harmful'],\n",
       " ['high', 'functioning', 'personality', 'disorder'],\n",
       " ['hit'],\n",
       " ['hit', 'wall'],\n",
       " ['hoarding'],\n",
       " ['hold', 'wall'],\n",
       " ['hold', 'minded'],\n",
       " ['holiday', 'trigger'],\n",
       " ['hoover', 'hoovering'],\n",
       " ['hospitalized', 'psychiatric', 'reason'],\n",
       " ['human', 'universal', 'also', 'cultural', 'universal'],\n",
       " ['humiliate'],\n",
       " ['humiliate', 'partner'],\n",
       " ['hurt'],\n",
       " ['hysteria'],\n",
       " ['identity', 'disturbance'],\n",
       " ['illicit', 'drug', 'use'],\n",
       " ['imposed', 'isolation'],\n",
       " ['impulsive'],\n",
       " ['impulsiveness'],\n",
       " ['inadequate', 'physical', 'activity'],\n",
       " ['inappropriate', 'touch'],\n",
       " ['inconsiderate'],\n",
       " ['independent'],\n",
       " ['indirect'],\n",
       " ['infantilization'],\n",
       " ['injure'],\n",
       " ['insecurity'],\n",
       " ['insult'],\n",
       " ['insult', 'boyfriend'],\n",
       " ['insult', 'girlfriend'],\n",
       " ['insult', 'partner'],\n",
       " ['intimate', 'partner', 'violence'],\n",
       " ['intimidate'],\n",
       " ['intimidation'],\n",
       " ['introvert'],\n",
       " ['introverted'],\n",
       " ['invalidation'],\n",
       " ['inventive'],\n",
       " ['irritating'],\n",
       " ['isolate', 'partner', 'family'],\n",
       " ['isolate', 'partner', 'friend'],\n",
       " ['isolation'],\n",
       " ['jealousy'],\n",
       " ['kick'],\n",
       " ['kick', 'door'],\n",
       " ['kick', 'furniture'],\n",
       " ['kick', 'wall'],\n",
       " ['lack', 'conscience'],\n",
       " ['lack', 'object', 'constancy'],\n",
       " ['lack', 'persistency'],\n",
       " ['low', 'achievement', 'orientation'],\n",
       " ['low', 'ambition'],\n",
       " ['low', 'communication'],\n",
       " ['low', 'empathy'],\n",
       " ['low', 'family', 'income'],\n",
       " ['low', 'functioning', 'personality', 'disorder'],\n",
       " ['low', 'innovation'],\n",
       " ['low', 'interest', 'education'],\n",
       " ['low', 'self', 'esteem'],\n",
       " ['magical', 'thinking'],\n",
       " ['making', 'fun'],\n",
       " ['manic'],\n",
       " ['manipulation'],\n",
       " ['manipulative'],\n",
       " ['manipulative', 'coercion'],\n",
       " ['many', 'sexual', 'partner'],\n",
       " ['masking'],\n",
       " ['mean', 'note'],\n",
       " ['military', 'clothes'],\n",
       " ['mirroring'],\n",
       " ['moment', 'clarity'],\n",
       " ['mood', 'swing'],\n",
       " ['moody'],\n",
       " ['moralistic'],\n",
       " ['munchausen', 'munchausen', 'proxy', 'syndrome'],\n",
       " ['name', 'calling'],\n",
       " ['narcissism'],\n",
       " ['neglect'],\n",
       " ['nervous'],\n",
       " ['nervous', 'excited'],\n",
       " ['win', 'scenario'],\n",
       " ['nonverbal', 'intimidation'],\n",
       " ['normalizing'],\n",
       " ['fault', 'syndrome'],\n",
       " ['using', 'seatbelt'],\n",
       " ['objectification'],\n",
       " ['obsessed', 'gun'],\n",
       " ['obsessive', 'compulsive', 'behavior'],\n",
       " ['oppositional', 'defiant', 'disorder'],\n",
       " ['ostracizing'],\n",
       " ['panic', 'attack'],\n",
       " ['paranoia'],\n",
       " ['parent', 'death'],\n",
       " ['parental', 'alienation', 'syndrome'],\n",
       " ['parentification'],\n",
       " ['participation', 'group', 'sex'],\n",
       " ['passive'],\n",
       " ['passive', 'aggression'],\n",
       " ['passive', 'aggressive', 'behavior'],\n",
       " ['pathological', 'lying'],\n",
       " ['perfectionism'],\n",
       " ['perfectionist'],\n",
       " ['persistent', 'nightmare'],\n",
       " ['physical', 'abuse'],\n",
       " ['physical', 'violence'],\n",
       " ['poor', 'adaptation', 'flexibility'],\n",
       " ['poor', 'detail', 'orientation'],\n",
       " ['poor', 'grade'],\n",
       " ['poor', 'intuition'],\n",
       " ['poor', 'leader'],\n",
       " ['poor', 'mental', 'health', 'score'],\n",
       " ['poor', 'persuasive', 'ability'],\n",
       " ['poor', 'team', 'player'],\n",
       " ['pornography', 'use'],\n",
       " ['posse', 'firearm'],\n",
       " ['possessive'],\n",
       " ['possessiveness'],\n",
       " ['pragmatic'],\n",
       " ['proactive'],\n",
       " ['projection'],\n",
       " ['prolonged', 'emotional', 'state'],\n",
       " ['proxy', 'recruitment'],\n",
       " ['psychological', 'distress'],\n",
       " ['psychosomatic', 'disorder'],\n",
       " ['push'],\n",
       " ['push', 'car'],\n",
       " ['push', 'partner'],\n",
       " ['push', 'pull'],\n",
       " ['put', 'partner'],\n",
       " ['raging', 'violence', 'impulsive', 'aggression'],\n",
       " ['random', 'violence'],\n",
       " ['ranking', 'comparing'],\n",
       " ['rape'],\n",
       " ['reactive'],\n",
       " ['rejected', 'peer'],\n",
       " ['relationship', 'hyper', 'vigilance'],\n",
       " ['reserved'],\n",
       " ['ridicule'],\n",
       " ['riding', 'emotional', 'elevator'],\n",
       " ['risk', 'taker'],\n",
       " ['robbery'],\n",
       " ['routine', 'oriented'],\n",
       " ['rude'],\n",
       " ['sabotage'],\n",
       " ['scapegoating'],\n",
       " ['school', 'suspension'],\n",
       " ['scratch'],\n",
       " ['scream', 'girlfriend'],\n",
       " ['selective', 'competence'],\n",
       " ['selective', 'memory', 'selective', 'amnesia'],\n",
       " ['self', 'aggrandizement'],\n",
       " ['self', 'harm'],\n",
       " ['self', 'loathing'],\n",
       " ['self', 'oriented'],\n",
       " ['self', 'victimization'],\n",
       " ['sense', 'entitlement'],\n",
       " ['serious'],\n",
       " ['sexual', 'assault'],\n",
       " ['sexual', 'coercion'],\n",
       " ['sexual', 'compulsivity'],\n",
       " ['sexual', 'harassment'],\n",
       " ['sexual', 'objectification'],\n",
       " ['sexually', 'active'],\n",
       " ['sexually', 'explicit', 'material'],\n",
       " ['shaming'],\n",
       " ['shy'],\n",
       " ['silent', 'treatment'],\n",
       " ['single', 'parent'],\n",
       " ['situational', 'ethic'],\n",
       " ['slam', 'wall'],\n",
       " ['slap'],\n",
       " ['sleep', 'deprivation'],\n",
       " ['smoking'],\n",
       " ['socially', 'withdrawn'],\n",
       " ['spend', 'time', 'friend'],\n",
       " ['spiteful'],\n",
       " ['splitting'],\n",
       " ['spread', 'hurtful', 'rumor'],\n",
       " ['stalking'],\n",
       " ['strangle'],\n",
       " ['stress', 'intolerant'],\n",
       " ['stunted', 'emotional', 'growth'],\n",
       " ['subject', 'negative', 'act', 'defend'],\n",
       " ['substance', 'use'],\n",
       " ['sudden', 'change', 'sleep', 'habbit'],\n",
       " ['sudden', 'weight', 'change'],\n",
       " ['suicidal'],\n",
       " ['suspension'],\n",
       " ['swear', 'partner'],\n",
       " ['take', 'drug'],\n",
       " ['targeted', 'humor', 'mocking', 'sarcasm'],\n",
       " ['tell', 'cruel', 'story'],\n",
       " ['temper', 'tantrum'],\n",
       " ['terminal', 'uniqueness'],\n",
       " ['terrorize', 'partner'],\n",
       " ['testing'],\n",
       " ['abuser', 'profile'],\n",
       " ['thought', 'policing'],\n",
       " ['thoughtless'],\n",
       " ['threat'],\n",
       " ['threat', 'partner'],\n",
       " ['threat'],\n",
       " ['throw', 'something'],\n",
       " ['twist', 'arm'],\n",
       " ['un', 'conventional', 'thinker'],\n",
       " ['uncooperative'],\n",
       " ['undependable'],\n",
       " ['undiplomatic'],\n",
       " ['unprotected', 'sex'],\n",
       " ['unwanted', 'sexual', 'touch'],\n",
       " ['victim'],\n",
       " ['visible', 'prolonged', 'sadness'],\n",
       " ['weak', 'school', 'bond'],\n",
       " ['weak', 'social', 'tie'],\n",
       " ['weapon', 'use'],\n",
       " ['yell', 'girlfriend'],\n",
       " ['leather', 'jacket'],\n",
       " ['disruptive', 'behavior'],\n",
       " ['vandalism'],\n",
       " ['problem', 'authority'],\n",
       " ['concentration', 'issue']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_volume(word_list, timesteps):\n",
    "    w2v = np.zeros([len(word_list), timesteps, 300])\n",
    "    c1 = 0\n",
    "    for word in word_list:\n",
    "        x = np.zeros([6, 300])\n",
    "        if type(word) == list:\n",
    "            c2 = 0\n",
    "            for w in word:\n",
    "                try: \n",
    "                    x[c2] = model[w]\n",
    "                    c2 += 1\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        elif type(word) == str:\n",
    "            x[0] = model[word]\n",
    "        w2v[c1] = x\n",
    "        c1 += 1\n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vol = input_volume(lemmatized_list, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input_vol', input_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
